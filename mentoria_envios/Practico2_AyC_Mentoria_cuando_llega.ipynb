{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oM70-GU7ZLSy"
   },
   "source": [
    "# Logística de envíos: ¿Cuándo llega?\n",
    "\n",
    "## Mentoría DiploDatos 2019 \n",
    "\n",
    "### Integrantes:\n",
    "\n",
    "- Alini, Walter\n",
    "- Frau, Johanna\n",
    "- Salina, Noelia\n",
    "\n",
    "### Mentora:\n",
    "\n",
    "- Dal Lago, Virginia\n",
    "\n",
    "### Práctico: Análisis exploratorio y curación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nOeFzK-zZcIM"
   },
   "source": [
    "## Motivación\n",
    "\n",
    "En la actualidad, cada vez más productos se comercializan a través de una plataforma online. Una de las principales ventajas de este sistema es que el usuario puede recibir el producto en su domicilio en una fecha determinada. Pero, ¿cómo sabemos qué día va a llegar? ¿A partir de qué datos podemos predecir la demora del envío? En este práctico se trabajará con datos de envíos de MercadoLibre, el e-commerce más grande de Latinoamérica, analizando y modelando el problema de logística de envíos para poder responder ¿cuándo llega?\n",
    "\n",
    "## Descripción del dataset\n",
    "\n",
    "**Datos:**  El conjunto de datos seleccionado para realizar el práctico corresponde a un muestreo aleatorio no uniforme de 500.000 envíos de MercadoLibre. Estos envíos fueron realizados en Brasil en el período comprendido entre Octubre de 2018 y Abril de 2019 (las fechas originales han sido modificadas y adaptadas a un período de tiempo diferente, conservando el día de la semana y considerando los feriados correspondientes). Mientras que las fechas han sido modificadas, los horarios registrados en el dataset son los originales. Los datos comprenden variables tanto categóricas como numéricas. \n",
    "\n",
    "El dataset cuenta con las siguientes columnas:\n",
    "\n",
    "- **Sender_state:** Estado de Brasil de donde sale el envío.\n",
    "- **Sender_zipcode:** Código postal (de 5 dígitos) de donde sale el envío.\n",
    "- **Receiver_state:** Estado de Brasil a donde llega el envío.\n",
    "- **Receiver_zipcode:** Código postal (de 5 dígitos) a donde llega el envío.\n",
    "- **Shipment_type:** Método de envío (normal, express, super).\n",
    "- **Quantity:** Cantidad de productos en un envío.\n",
    "- **Service:** Servicio del correo con el cual se realizó un envío.\n",
    "- **Status:** Estado del envío (set: listo para ser enviado, sent: enviado, done: entregado, failed: no entregado, cancelled: cancelado).\n",
    "- **Date_created:** Fecha de creación del envío.\n",
    "- **Date_sent:** Fecha y hora en que se realizó el envío (salió del correo).\n",
    "- **Date_visit:** Fecha y hora en que se entregó el envío al destinatario.\n",
    "- **Shipment_days:** Días hábiles entre que el envío fue enviado (salió del correo) y que fue entregado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0aMgSG5NZm55"
   },
   "source": [
    "## Objetivos generales\n",
    "\n",
    "\n",
    "  * Realizar un estudio exploratorio del dataset para extraer información útil sobre el problema a resolver\n",
    "  * Desarrollar visión crítica en relación a la problemática para llevar a cabo el procedimiento de ciencia de datos\n",
    "  * Desarrollar habilidades de comunicación de la información obtenida a partir de los datos de manera clara y sencilla.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06srV32U2q7J"
   },
   "source": [
    "## Objetivos específicos\n",
    "\n",
    "ToDo habría que agregar objetivos especificos una vez que tengamos mas avanazado el practico como sugirio la vir en las corecciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-XE83TKaPDv"
   },
   "source": [
    "## Metodología\n",
    "\n",
    "A lo largo de este trabajo trataremos de responder a las siguientes preguntas:\n",
    "\n",
    "1. ¿Existen datos duplicados? ¿Con qué información deberíamos contar para poder determinar con certeza que dichos datos están duplicados y no corresponden a dos envíos diferentes?\n",
    "1. En relación a las diferentes columnas de fechas, en el práctico anterior notamos que muchas de ellas no eran consistentes. Identificar y cuantificar los valores atípicos de cada columna. ¿Qué tipo de anomalías hay en el conjunto completo? ¿Qué deberíamos realizar con dichos datos?\n",
    "1. Una información que, a futuro, podría ser relevante para modelar es el estado de Brasil al cual llega el envío. Sin embargo, esta información está codificada en la columna en formato de texto. ¿De qué maneras podríamos transformar la misma en valores numéricos para utilizar en un modelo? Mostrar al menos dos ejemplos.\n",
    "1. A la hora de determinar la promesa de entrega de un envío (fecha estimada de llegada), ¿cuáles son los features que consideran pueden tener mayor relevancia? ¿Cuál es el valor a predecir?\n",
    "1. Suponiendo que queremos emplear un modelo kNN para calcular la promesa de entrega de un envío. ¿Qué transformaciones sugieren realizar sobre los features antes seleccionados? Mostrar al menos un ejemplo.\n",
    "1. Nos interesa determinar si un envío llegará entre 0-1 días, 2-3 días, 4-5 días, 6-7 días, 8-9 días ó 10 o más días. Construir una nueva columna en el dataset que indique a cuál de estos grupos pertence cada envío, y codificarla numéricamente, de manera tal de poder ser utilizada como valor a predecir. Seleccionar los dos features que se consideren de mayor relevancia para un modelo, y aplicar un procedimiento de clustering (separando en los grupos antes definidos).\n",
    "\n",
    "Esta comunicación debe estar dirigida para un público técnico pero que desconoce los aspectos propios del problema a resolver (por ejemplo, sus compañeros de clase). Se evaluará, principalmente, la claridad del mensaje presentado, el uso de las herramientas y los conceptos desarrollados en las clases teóricas. \n",
    "Además se debe realizar una breve comunicación en pdf (2 páginas máximo) dirigida a un stakeholder del proyecto (por ejemplo, manager), comentando los hallazgos y problemas encontrados, y las posibles acciones a tomar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOIdddL4bOfA"
   },
   "source": [
    "## Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DFLvheh7b0xL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy import special\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import category_encoders as ce # usar \"pip install category_encoders\" para instalar\n",
    "\n",
    "BLUE = '#35A7FF'\n",
    "RED = '#FF5964'\n",
    "GREEN = '#6BF178'\n",
    "YELLOW = '#FFE74C'\n",
    "\n",
    "RELATIVE_PATH = '../diplodatos_2019/mentoria_envios/'\n",
    "DATA_FILE = 'dataset_sample_corrected.csv'\n",
    "\n",
    "# Establecemos una semilla por cuestiones de reproducibilidad\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GyHVCkkqb8Dz"
   },
   "source": [
    "### Lectura y análisis inicial de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59nP6TMibpTL"
   },
   "outputs": [],
   "source": [
    "ds = pd.read_csv(RELATIVE_PATH + DATA_FILE, \n",
    "                       dtype={'sender_zipcode':'int64',\n",
    "                              'receiver_zipcode':'int64',\n",
    "                              'quantity':'int64',\n",
    "                              'service':'int64'},\n",
    "                       parse_dates=['date_created','date_sent','date_visit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "K9cpWdHa3ae4",
    "outputId": "80683362-f59e-4099-fee4-64d1168e090f"
   },
   "outputs": [],
   "source": [
    "ds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "NXEwu3Be3s1y",
    "outputId": "84b7e295-723f-44aa-8834-fd469f7b0ef5"
   },
   "outputs": [],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "colab_type": "code",
    "id": "7MyczHXg3wkN",
    "outputId": "5140f703-b4df-49a6-cc8f-ffedb4d1bc40",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) #con propositos de reproducibilidad\n",
    "ds.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zodgXxGv4ENd"
   },
   "source": [
    "###  Shipment_days anómalos\n",
    "\n",
    "En el práctico anterior habiamos tomado la decisión de no trabajar con valores de la variable **shipment_days** negativos teniendo en cuenta que no los podemos considerar valores reales y que la proporción de estos datos dentro del conjunto total es muy chica comparada con la cantidad total.\n",
    "\n",
    "Por lo tanto nuestra primer medida será aplicar este filtro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "MsAY5C3p47tT",
    "outputId": "20984f68-e66f-4ce4-f6ef-ac20870a9fe1"
   },
   "outputs": [],
   "source": [
    "m_shipment_days_negative = ds.shipment_days < 0\n",
    "m_shipment_days_positive = ~ m_shipment_days_negative\n",
    "\n",
    "ds[m_shipment_days_negative].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "JVOxxCFT5PqI",
    "outputId": "9c03a1d7-92ec-449d-de26-4f0a0677ad5e"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "ds[m_shipment_days_positive].sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "AjJ3U-gk5Zd7",
    "outputId": "e833e269-263b-4ba6-b9d2-76f0bc88d9d3"
   },
   "outputs": [],
   "source": [
    "dataset = ds[m_shipment_days_positive]\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivG2vKdQcW3o"
   },
   "source": [
    "### Datos duplicados\n",
    "\n",
    "A partir de la información que tenemos del dataset (ver \"Descripción del dataset\") no se puede identificar una columna que pueda ser tomada como valor único para una fila del dataset. La única columna que quizá pueda tomar esa función, sería **date_created**. Pero, como vimos en el práctico anterior, esta variable sólo tiene fecha (y no hora) de la creación del evento, con lo cual no tiene sentido tomarla como \"id\" en nuestro dataset.\n",
    "\n",
    "Con lo cual, una siguiente hipótesis sería tomar a todos los datos del dataset como el identificador de un envío en particular. Veamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2810
    },
    "colab_type": "code",
    "id": "-SOY59mccEep",
    "outputId": "08616d12-e7f0-49eb-85db-ffc5b480fff9"
   },
   "outputs": [],
   "source": [
    "ds[ds.duplicated(keep=False)].sort_values('date_created', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xABkbQ9ivWw"
   },
   "source": [
    "Tenemos entonces 11863 filas del dataset (2.37%) que son iguales, dato a dato, a alguna otra fila. La pregunta que corresponde hacerse es: ¿Es suficiente saber que dos filas son iguales dato a dato para considerarlas iguales, o podríamos considerar por error que son datos iguales cuando corresponden a envíos distintos?\n",
    "\n",
    "A priori, creemos que no hay una respuesta que podamos dar que tenga una certidumbre razonable para tomar una decisión, sin contar con el aporte de información de expertos en el dominio (esto es, personas que sepan cómo estos datos fueron tomados o generados, qué tipo de error tiene cada uno de los datos, cuáles datos son generados automáticamente y cuáles manualmente, etc.) o información en el dataset que sean indicadores determinado para esta unicidad (como por ejemplo, un identificador -una o más variables del dataset- declarado como tal).\n",
    "\n",
    "Intentaremos sumar un poco más de información a partir de la exploración del dataset y de los (potencialmente) duplicados para tomar una decisión informada:\n",
    "\n",
    "1. Lo primero que vemos es que tanto **date_sent** como **date_received** tienen precisión al segundo. Podríamos asumir que aquellas entradas con el valor del segundo en 0 para algunos de estos campos podrían responder a que la forma de carga toma información hasta el minuto, pero vemos ocurrencias de entradas con segundos distintos de 0, con lo cual esta asunción no tendría mucho sustento (o al menos no lo tendría para muchos de los casos).\n",
    "Con lo cual, la probabilidad de que dos paquetes que salen del mismo estado, van al mismo estado, usan el mismo correo, y llegan y son recibidos a una misma fecha (con precisión de segundos, más allá de si este dato en particular se refiere al momento de carga o al momento preciso de envío o recepción) consideramos que es realmente baja. Este punto apoyaría la moción de considerar como repetidas estas entradas.\n",
    "\n",
    "2. Para el caso de envíos con **quantity** mayor a 1, esta moción se reforzaría, ya que los envíos con más de un paquete son una parte minoritaria de los envíos, y la probabilidad de que no fuera un duplicado sería aún más baja.\n",
    "\n",
    "3. Mismo análisis que el punto anterior se puede hacer para el caso de **shipment_type** super express.\n",
    "\n",
    "A partir de este análisis, creemos que tiene sentido considerar a las filas que comparten el mismo valor para todas sus columnas como repetidas y no como envíos distintos. O, en todo caso, considerarlos como datos que no son confiables para la problemática que estamos intentando atacar.\n",
    "\n",
    "Siendo que estas filas pueden estar repetidas más de dos veces, intentemos cuantificar de cuántas estamos hablando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2843
    },
    "colab_type": "code",
    "id": "9As2gRT5gneP",
    "outputId": "47194d8a-7388-4ee3-f8f8-2b38211945c8"
   },
   "outputs": [],
   "source": [
    "ds[ds.duplicated()].sort_values('date_created', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SrJVI5wonNSg"
   },
   "source": [
    "Listando sólo una ocurrencia de las filas repetidas (en este caso la 1era, de acuerdo a cómo funciona *duplicated*) tenemos 6483 filas únicas del original 11863 que incluyen todas las repetidas. Esto nos da un total de 5380 filas que estaríamos considerando repetidas y debemos quitar del dataset como parte de nuestro proceso de curación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jc-r4WKTpk1p"
   },
   "source": [
    "### Datos faltantes\n",
    "\n",
    "Lo siguiente que vamos a revisar, como lo hicimos en el práctico anterior, son los datos faltantes (para cualquiera de las columnas del dataset). Intentaremos listarnos, buscar una explicación de los mismos, y decidir si (y cómo) lidiamos con ellos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "8a_YI1lfo546",
    "outputId": "158f8fe4-3016-4cde-8e86-c8adf384f3f0"
   },
   "outputs": [],
   "source": [
    "ds.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePjZOyVzr8Z0"
   },
   "source": [
    "Pandas reconoce 1233 valores faltantes, en las variables **date_sent** (29), **date_visit** (602) y **shipment_days** (602). Esperaríamos a priori que:\n",
    "1. Las filas con **shipment_days** faltantes y las filas con **date_visit** faltantes sean las mismas;\n",
    "1. Que los valores faltantes se encuentren en envíos con un **status** que explique este faltante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "6MDQyVQ5zfUz",
    "outputId": "fb9920e7-5a3f-43f9-fd2b-ee26efe9cd1a"
   },
   "outputs": [],
   "source": [
    "filter_1 = ds.shipment_days.isnull() | ds.date_visit.isnull()\n",
    "len(ds[filter_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pkrIwAVU5Jpb"
   },
   "source": [
    "Validamos el primer supuesto. Veamos el segundo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "t1KdNSKY5OI0",
    "outputId": "dfd38f87-2b6e-47d0-c2d2-882d210b5c01"
   },
   "outputs": [],
   "source": [
    "filter_2 = ds.date_sent.isnull() | ds.date_visit.isnull()\n",
    "ds[filter_2].status.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6xj0Tez5kCI"
   },
   "source": [
    "No validamos el segundo supuesto, llama la atención los envíos en \"done\" con estas condiciones, con lo que podemos concluir que pueden ser anómalos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "colab_type": "code",
    "id": "alotWPDg54b1",
    "outputId": "b8cc4800-81b6-4df6-8ba8-d0a3c9e24516",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds[filter_2][ds.status == 'done']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUKdLem9zcIB"
   },
   "source": [
    "Intentaremos ver si existen otros valores \"faltantes\" que Pandas no esté reconociendo como tales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1616
    },
    "colab_type": "code",
    "id": "WKEwviIAskNT",
    "outputId": "c2493215-f3b8-4cb5-f567-a5e420f4f4bb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ToDo: Esto se puede hacer un poco más prolijo\n",
    "for col in ds:\n",
    "    print (ds[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjc1hQCDuzfu"
   },
   "source": [
    "A simple vista, pareciera que todos los valores faltantes están considerados en el análisis anterior.\n",
    "\n",
    "Dicho esto, y con el contexto de la problemática que deseamos resolver, creemos que es conveniente eliminar las filas con datos faltantes: responden a datos que no nos servirán de información para la estimación de envío (son envíos que no llegaron, por diversas razones) o son datos que consideramos anómalos para el dataset. Cualquier política para completar esos datos con algún valor distinto (persistiendo la fila en cuestión) creemos que nos llevaría a tener información no confiable en el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IOn642wx8gze"
   },
   "source": [
    "### Datos anómalos\n",
    "\n",
    "Más allá de los valores anómalos mencionados en la sección anterior, intentaremos identificar otras anomalías o inconsistencias en el data set.\n",
    "\n",
    "**ToDo: Acá copiaría exactamente lo que hicimos el práctico anterior (\"Análisis de shipment_days anómalos\" y \"Análisis de fechas anómalas\")**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dw-pGAQ2GmI-"
   },
   "source": [
    "### Análisis de fechas anómalas (Parte I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmpXCI0ZGtP4"
   },
   "source": [
    "Las 3 variables de fechas que tenemos en este dataset, y los supuestos que proponemos para ellas, son:\n",
    "\n",
    "- **date_created**: Corresponde a la fecha de creación del envío.\n",
    "  - Es una fecha que se crea automáticamente a través de algunos de los procesos de venta de MercadoLibre.\n",
    "  - Sólo nos interesa la fecha y no la hora de creación\n",
    "- **date_sent**: Corresponde a la fecha y hora en que alguno de los correos cargó para el envío en cuestión, que comenzó el proceso de envío\n",
    " - Consideraremos **date_sent** como la fecha en que el envío está a cargo del correo y no necesariamente cuando el envío \"salió\" del correo (más bien cuando \"entró\" al correo, que es cuando el proceso de handling llega a su fin).\n",
    " - Puede crearse \"automáticamente\" cuando el vendedor deja el contenido del envío en el correo.\n",
    "  - Puede también crearse \"manualmente\", pensando en lugares de recepción de correos sin la tecnología suficiente para que esta fecha sea generada automáticamente. Es posible entonces que sea necesario \"aceptar\" algunas fechas no precisas como válidas.\n",
    " * Por los supuestos en **date_created** y las anteriores para **date_sent**, esta fecha debe necesariamente ser posterior a **date_created**\n",
    "- **date_visit**: Corresponde a la fecha y hora en que alguno de los correos cargó en envío en cuestión como entregado.\n",
    "  - Puede crearse \"automáticamente\" cuando el cartero utiliza tecnología (lector de código de barras o aplicación movil, por ejemplo).\n",
    "  - Puede crearse \"manualmente\", pensando en que el cartero, al terminar un ciclo o jornada, provee de la información de todos los envíos entregados en ese ciclo o jornada.\n",
    "  - Por los supuestos en **date_created**, **date_sent** y los anteriores de **date_visit**, esta fecha debe neceariamente ser posterior a **date_created**. Se espera también que sea posterior a **date_sent** también, pero deberemos tomar una decisión en estos casos en función de análisis del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "StgsBIXUGiDt",
    "outputId": "99309757-f259-43b3-d64c-a5e643bc983d"
   },
   "outputs": [],
   "source": [
    "m_date_created_after_date_sent = (ds.date_created >= ds.date_sent)\n",
    "m_date_created_before_date_sent = (ds.date_created < ds.date_sent)\n",
    "\n",
    "m_date_created_after_date_visit = (ds.date_created >= ds.date_visit)\n",
    "m_date_created_before_date_visit = (ds.date_created < ds.date_visit)\n",
    "\n",
    "m_date_created_invalid = m_date_created_after_date_sent | \\\n",
    "                         m_date_created_after_date_visit\n",
    "\n",
    "ds[m_date_created_invalid].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3VCcDFWqG43M"
   },
   "source": [
    "Tenemos 194 casos donde los datos no siguen los supuestos planteados. Por lo tanto, consideramos que esta información anómala no es confiable y deberíamos eliminarla del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "KASIW1-IG9sV",
    "outputId": "0e9cfb83-7121-46a1-b32d-6e9b0928986c"
   },
   "outputs": [],
   "source": [
    "m_date_sent_after_date_visit = ds.date_sent >= ds.date_visit\n",
    "m_date_sent_before_date_visit = ds.date_sent < ds.date_visit\n",
    "\n",
    "ds[m_date_sent_after_date_visit].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8-B7jRtHEd2"
   },
   "source": [
    "Tenemos 2407 casos en donde se consignó que el paquete entró al proceso de envío después de la fecha de consignación de llegada. No representan una cantidad significativa de casos como para analizar si tiene sentido sacarlos o dejarlos del dataset, por lo que consideramos que es información no confiable que podemos quitar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4T4PUftK5qn7"
   },
   "source": [
    "### Análisis de fechas anómalas (Parte II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "id": "GbhIcs7y55pf",
    "outputId": "17ed3c9f-708d-4b63-cc4f-7697e173f071"
   },
   "outputs": [],
   "source": [
    "dataset.describe(include='datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2f0KOIi5-YM"
   },
   "source": [
    "En el práctico anterior habiamos observado lo siguiente:\n",
    "\n",
    "* La variables **date_created** solo tiene 144 valores distintos y no considera horas (todas están seteadas a las 00:00 hs) esto implica que estamos considerando un total de 144 días distintos.\n",
    "* La variable **date_created**  contienen fechas que caen fuera del intervalo temporal considerado y más aún, que representa una fecha futura (Junio de 2019).\n",
    "* Hay una cierta variabilida entre los mínimos de las 3 fechas, podría servir analizar los estados de las fechas fuera del periodo de **date_visit** para ver si no son anómalos.\n",
    "* En la variable **date_visit** el dato más frecuente se da a las 22.00 hs, posiblemente fuera del horario de atención de los correos, con lo cual este dato es bastante extraño en un principio. Lo mismo sucede con el horario 05:36.\n",
    "\n",
    "Teniendo en cuenta la problemática que queremos resolver, la primera observación cobra cierto sentido pues el día de creación del envío resulta mucho más importante que la hora en si misma. Además, considerando que la respuesta del problema en un principio debe consistir en dar un intervalo de días, las franjas horarias podrían no ser tenidas en cuenta a priori.\n",
    "\n",
    "Con respecto a la segunda observación podríamos intentar determinar la cantidad de fechas que caen fuera del intervalo Octubre de 2018-Abril de 2019 y al mismo tiempo observar la tendencia a medida que pasan los meses de las variables **date_created**, **date_sent** y **date_visited**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "iCtXPd4V7M7Q",
    "outputId": "ea2b8f7f-39e5-4c02-af3c-d47dcc280572"
   },
   "outputs": [],
   "source": [
    "dataset['date_created'].groupby(\n",
    "    [dataset['date_created'].dt.year.rename('year'),\n",
    "     dataset['date_created'].dt.month.rename('month')]).agg({'count'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VyofGHJJ7QOt"
   },
   "source": [
    "**Observaciones:**\n",
    "\n",
    "* Tenemos una fecha de creación en el mes de septiembre y 111 fechas en el mes de Junio . Todas ellas caen fuera del intervalo considerado.\n",
    "\n",
    "* La mayor concentración en cuanto a fechas de creación de envíos se dió en la franja enero-marzo, donde marzo cuenta con más de la mitad de los datos. \n",
    "\n",
    "**Preguntas y comentarios a tener en cuenta:**\n",
    "\n",
    "1. ¿Es normal que en un intervalo de 6 meses los datos se concentren mayormente en un mes o dos?\n",
    "2. ¿Podemos pensar que los datos de septiembre de 2018 y julio de 2019 hayan tenido que ver con errores de carga de los datos y sean efectivamente datos que se encuentran en el intervalo considerado? ¿O efectivamente serán datos que tenemos que eliminar de nuestro conjunto de análisis?\n",
    "3. Los 111 datos en el mes de junio resultan bastante extraños y quizás merecen un análisis más detallado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "id": "kiZL4B-P7hyn",
    "outputId": "3e9838d6-12de-4249-981a-8a0e225520ed"
   },
   "outputs": [],
   "source": [
    "dataset['date_sent'].groupby(\n",
    "    [dataset['date_sent'].dt.year.rename('year'),\n",
    "     dataset['date_sent'].dt.month.rename('month')]).agg({'count'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "46kqptYs7m4H"
   },
   "source": [
    "**Observaciones:**\n",
    "\n",
    "No se observan datos fuera del intervalo considerando pero si  una gran concentración en el intervalo enero-marzo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "lQUlgv9B8Vsh",
    "outputId": "949e1b40-9b59-4afd-ecfd-08d21ada18de"
   },
   "outputs": [],
   "source": [
    "dataset['date_visit'].groupby(\n",
    "    [dataset['date_visit'].dt.year.rename('year'),\n",
    "     dataset['date_visit'].dt.month.rename('month')]).agg({'count'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfh3Si1L8cDc"
   },
   "source": [
    "**Observaciones:**\n",
    "\n",
    "* No se observan datos anómalos en la variable **date_visit**.\n",
    "* Resulta extraño que tengamos fecha de visita solo en los meses de febrero, marzo y abril cuando nuestro intervalo temporal es más grande\n",
    "* Estos datos merecen un análisis más detallado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lz8z4s_86ae"
   },
   "source": [
    "### Análisis de la variable date_created\n",
    "\n",
    "Ahora vamos a analizar las 111 fechas de creación del mes de junio y el valor del mes de septiembre para ver si encontramos alguna anomalia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "colab_type": "code",
    "id": "rmBGNb2mAvCC",
    "outputId": "1829373b-1126-4cd3-967a-a07dd9d6ac5a"
   },
   "outputs": [],
   "source": [
    "date_created_septiembre = dataset['date_created'].map(lambda x: x.month) == 9\n",
    "dataset[date_created_septiembre]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmWnxl_yCRDb"
   },
   "source": [
    "Consideramos que este dato está mal cargado y su presencia no aporta valor a nuestro análisis. Por lo tanto procederemos a eliminarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "colab_type": "code",
    "id": "EnxfR7bCC_2T",
    "outputId": "f1abeb0e-ac5e-4a37-f725-08229510f687"
   },
   "outputs": [],
   "source": [
    "dataset.drop(dataset[dataset['date_created'].map(lambda x: x.month) == 9].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "syP-MgY8SiJv",
    "outputId": "37ff2fe8-2ea6-4bcd-a848-de5a54694d79"
   },
   "outputs": [],
   "source": [
    "dataset[date_created_septiembre] #Corroboramos que se haya eliminado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2926
    },
    "colab_type": "code",
    "id": "EAHqY92E9GrC",
    "outputId": "0ae57339-475d-4c99-c747-96025ce183b1"
   },
   "outputs": [],
   "source": [
    "date_created_junio = dataset['date_created'].map(lambda x: x.month) == 6\n",
    "dataset[date_created_junio]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPfLJ2wFPEob"
   },
   "source": [
    "Si bien estos datos caen fuera de los supuestos en cuanto a que la fecha de creación debe ser anterior a la fecha de envío, la inspección de los datos de arriba  nos inclina a pensar que los datos con fecha de creación en junio de 2019 en realidad corresponden a datos del mes de enero del mismo año. De esa manera, las columnas de las fechas parecerian tener más coherencia y cumplir con los supuestos planteados. Bajo esta hipótesis nos encontramos frente a la diyuntiva de si eliminamos esta información o imputamos los datos haciendo que su fecha de creación sea  enero de 2019.\n",
    "\n",
    "Si uno se guía por los datos como vienen la primera opción sería estas filas (en cuanto a cantidad no afectaría a la cantidad total) sin embargo podemos tomar la jugada arriesgada y \"modificar\" nuestros datos subsanando de esta manera en posible error producido durante la carga de los datos. \n",
    "\n",
    "A continuación reemplazaremos los datos de creación de junio de 2019 por enero de 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVJ5trRdHX9F"
   },
   "source": [
    "## Dataset limpio\n",
    "\n",
    "#ToDo \n",
    "(habría que charlar esta parte en base a la parte II de arriba)\n",
    "\n",
    "En función de lo visto en las secciones anteriores, generaremos un nuevo dataset con las siguientes características:\n",
    "\n",
    "- Sin datos \"potencialmente duplicados\" (ver \"Datos duplicados\")\n",
    "- Sin datos nulos (ver \"Datos faltantes\")\n",
    "- Sin **shipment_days** negativos (ver \"Análisis de shipment_days anómalos\")\n",
    "- Sin envíos creados después de ser enviados (ver \"Análisis de fechas anómalas\")\n",
    "- Sin envíos creados después de ser recibidos (ver \"Análisis de fechas anómalas\")\n",
    "- Sin envíos enviados después de ser recibidos (ver \"Análisis de fechas anómalas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "-tfLr6krHOzw",
    "outputId": "8166db3b-951b-41d4-b310-b2b1c2d06977"
   },
   "outputs": [],
   "source": [
    "# Generamos un nuevo dataset \"limpio\"\n",
    "\n",
    "# 1. Sin datos \"potencialmente duplicados\"\n",
    "dataset = ds.drop_duplicates()\n",
    "\n",
    "# 2. Sin datos nulos\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# 3. Sin shipment_days negativos\n",
    "dataset = dataset[m_shipment_days_positive]\n",
    "\n",
    "# 4. Sin envíos creados después de ser enviados\n",
    "dataset = dataset[m_date_created_before_date_sent]\n",
    "\n",
    "# 5. Sin envíos creados después de ser recibidos\n",
    "dataset = dataset[m_date_created_before_date_visit]\n",
    "\n",
    "# 6. Sin envíos enviados después de ser recibidos\n",
    "dataset = dataset[m_date_sent_before_date_visit]\n",
    "\n",
    "dataset.to_csv(RELATIVE_PATH + \"data_sample_corrected_cleaned.csv\", sep=',', index=False)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZiiYNe8_KVbZ"
   },
   "source": [
    "Resultan 490.439 entradas (de las 500.000 originales), con lo cual hemos dejado en el camino menos del 2% del dataset original, un número que, por la confianza que nos transmiten esos datos, podemos anticipar que es razonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hsfs4IYZZy0q"
   },
   "source": [
    "## Pregunta 3\n",
    "\n",
    "Para convertir datos categóricos en numéricos se pueden usar, entre otros, los siguientes dos métodos:\n",
    "* **One-Hot encoding:** traduce cada valor categórico en una columna independiente con valor 1 en los registros que tenían dicha categoría. Este método, si bien no tiene problemas con los \"pesos\" que pueden heredar las categorías, puede presentar problemas cuando el número total de categorías es grande, ya que los datos se codifican en grandes dimensiones.\n",
    "* **Binary Encoding:** toma el índice ordinal de las categorías y las codifica de forma binaria, donde los bits de esa codificación se encuentran separados en columnas. La ventaja que tiene respecto al método anterior es que los datos se codifican en menores dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "colab_type": "code",
    "id": "FD-wY1YPKvdp",
    "outputId": "de40df73-7767-48c2-9fc3-51c6edaaeb7f"
   },
   "outputs": [],
   "source": [
    "dataset['receiver_state'] = pd.Categorical(dataset['receiver_state'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hsfs4IYZZy0q"
   },
   "source": [
    "**One-Hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "32Iw8bH9fcUD"
   },
   "outputs": [],
   "source": [
    "dfDummies = pd.get_dummies(dataset['receiver_state'], prefix = 'category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8yqnrRBfnbf"
   },
   "outputs": [],
   "source": [
    "dataset_dummies = pd.concat([dataset,dfDummies], axis=1)\n",
    "dataset_dummies.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ce = dataset.copy()\n",
    "\n",
    "encoder = ce.BinaryEncoder(cols=['receiver_state'])\n",
    "dataset_binary = encoder.fit_transform(dataset_ce)\n",
    "\n",
    "dataset_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mentoría Envíos - Práctico 2",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
